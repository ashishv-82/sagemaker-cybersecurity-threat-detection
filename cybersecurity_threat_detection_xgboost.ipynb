{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d72793d",
   "metadata": {},
   "source": [
    "## Step 1: Load Raw Data from S3\n",
    "\n",
    "Download the UNSW-NB15 dataset from AWS S3. This dataset contains network traffic features with binary labels (benign vs. attack).\n",
    "- **Source**: S3 bucket `cybersecurity-data-for-training`\n",
    "- **File**: `raw-data/UNSW_NB15_training-set.csv`\n",
    "- **Output**: Pandas DataFrame with raw network traffic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac864a49-d0cd-4799-bd4c-8ad31a280bb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(175341, 45)\n",
      "Index(['id', 'dur', 'proto', 'service', 'state', 'spkts', 'dpkts', 'sbytes',\n",
      "       'dbytes', 'rate', 'sttl', 'dttl', 'sload', 'dload', 'sloss', 'dloss',\n",
      "       'sinpkt', 'dinpkt', 'sjit', 'djit', 'swin', 'stcpb', 'dtcpb', 'dwin',\n",
      "       'tcprtt', 'synack', 'ackdat', 'smean', 'dmean', 'trans_depth',\n",
      "       'response_body_len', 'ct_srv_src', 'ct_state_ttl', 'ct_dst_ltm',\n",
      "       'ct_src_dport_ltm', 'ct_dst_sport_ltm', 'ct_dst_src_ltm',\n",
      "       'is_ftp_login', 'ct_ftp_cmd', 'ct_flw_http_mthd', 'ct_src_ltm',\n",
      "       'ct_srv_dst', 'is_sm_ips_ports', 'attack_cat', 'label'],\n",
      "      dtype='object')\n",
      "   id       dur proto service state  spkts  dpkts  sbytes  dbytes       rate  \\\n",
      "0   1  0.121478   tcp       -   FIN      6      4     258     172  74.087490   \n",
      "1   2  0.649902   tcp       -   FIN     14     38     734   42014  78.473372   \n",
      "2   3  1.623129   tcp       -   FIN      8     16     364   13186  14.170161   \n",
      "3   4  1.681642   tcp     ftp   FIN     12     12     628     770  13.677108   \n",
      "4   5  0.449454   tcp       -   FIN     10      6     534     268  33.373826   \n",
      "\n",
      "   ...  ct_dst_sport_ltm  ct_dst_src_ltm  is_ftp_login  ct_ftp_cmd  \\\n",
      "0  ...                 1               1             0           0   \n",
      "1  ...                 1               2             0           0   \n",
      "2  ...                 1               3             0           0   \n",
      "3  ...                 1               3             1           1   \n",
      "4  ...                 1              40             0           0   \n",
      "\n",
      "   ct_flw_http_mthd  ct_src_ltm  ct_srv_dst  is_sm_ips_ports  attack_cat  \\\n",
      "0                 0           1           1                0      Normal   \n",
      "1                 0           1           6                0      Normal   \n",
      "2                 0           2           6                0      Normal   \n",
      "3                 0           2           1                0      Normal   \n",
      "4                 0           2          39                0      Normal   \n",
      "\n",
      "   label  \n",
      "0      0  \n",
      "1      0  \n",
      "2      0  \n",
      "3      0  \n",
      "4      0  \n",
      "\n",
      "[5 rows x 45 columns]\n",
      "label\n",
      "1    119341\n",
      "0     56000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import io\n",
    "\n",
    "# Setup S3 client\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "# Download the file into memory\n",
    "response = s3_client.get_object(Bucket='cybersecurity-data-for-training', Key='raw-data/UNSW_NB15_training-set.csv')\n",
    "\n",
    "# Read it into pandas\n",
    "df = pd.read_csv(io.BytesIO(response['Body'].read()))\n",
    "\n",
    "# Explore\n",
    "print(df.shape)\n",
    "print(df.columns)\n",
    "print(df.head())\n",
    "print(df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0bed78c",
   "metadata": {},
   "source": [
    "## Step 2: Data Preprocessing & Feature Engineering\n",
    "\n",
    "Prepare the data for machine learning:\n",
    "1. **Drop irrelevant columns**: Remove 'id' and 'attack_cat' (not useful for classification)\n",
    "2. **Feature engineering**: Create new features that capture network behavior:\n",
    "   - `byte_ratio`: Ratio of source to destination bytes\n",
    "   - `is_common_port`: Binary indicator for common ports (80, 443, 22)\n",
    "   - `flow_intensity`: Packets per unit time (network activity rate)\n",
    "3. **One-hot encoding**: Convert categorical features (proto, service, state) into numeric form\n",
    "4. **Standardization**: Scale all numerical features to mean=0, std=1 (important for XGBoost convergence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee73cbbc-2194-4403-8b49-5d424ebe027d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(175341, 198)\n",
      "        dur     spkts     dpkts    sbytes    dbytes      rate      sttl  \\\n",
      "0 -0.191029 -0.104456 -0.135769 -0.049134 -0.102726 -0.576371  0.703839   \n",
      "1 -0.109485 -0.046014  0.172599 -0.046410  0.188544 -0.576345 -1.141901   \n",
      "2  0.040699 -0.089845 -0.026933 -0.048527 -0.012133 -0.576734 -1.141901   \n",
      "3  0.049729 -0.060624 -0.063212 -0.047016 -0.098563 -0.576737 -1.141901   \n",
      "4 -0.140417 -0.075235 -0.117630 -0.047554 -0.102057 -0.576617  0.723268   \n",
      "\n",
      "       dttl     sload     dload  ...  service_ssl  state_CON  state_ECO  \\\n",
      "0  1.578100 -0.389897 -0.273700  ...    -0.017874  -0.284764  -0.008273   \n",
      "1  1.560002 -0.389928 -0.069233  ...    -0.017874  -0.284764  -0.008273   \n",
      "2  1.560002 -0.389964 -0.252044  ...    -0.017874  -0.284764  -0.008273   \n",
      "3  1.560002 -0.389958 -0.275821  ...    -0.017874  -0.284764  -0.008273   \n",
      "4  1.560002 -0.389927 -0.275561  ...    -0.017874  -0.284764  -0.008273   \n",
      "\n",
      "   state_FIN  state_INT  state_PAR  state_REQ  state_RST  state_URN  state_no  \n",
      "0   1.119382  -0.940239  -0.002388   -0.10717  -0.021762  -0.002388 -0.002388  \n",
      "1   1.119382  -0.940239  -0.002388   -0.10717  -0.021762  -0.002388 -0.002388  \n",
      "2   1.119382  -0.940239  -0.002388   -0.10717  -0.021762  -0.002388 -0.002388  \n",
      "3   1.119382  -0.940239  -0.002388   -0.10717  -0.021762  -0.002388 -0.002388  \n",
      "4   1.119382  -0.940239  -0.002388   -0.10717  -0.021762  -0.002388 -0.002388  \n",
      "\n",
      "[5 rows x 198 columns]\n",
      "                   mean       std\n",
      "dur       -3.241878e-18  1.000003\n",
      "spkts     -1.426426e-17  1.000003\n",
      "dpkts      5.187005e-18  1.000003\n",
      "sbytes    -1.053610e-18  1.000003\n",
      "dbytes     1.410217e-17  1.000003\n",
      "...                 ...       ...\n",
      "state_PAR -1.347406e-18  1.000003\n",
      "state_REQ  3.930777e-18  1.000003\n",
      "state_RST -2.917690e-18  1.000003\n",
      "state_URN  7.902078e-19  1.000003\n",
      "state_no   7.902078e-19  1.000003\n",
      "\n",
      "[198 rows x 2 columns]\n",
      "dur         -0.0\n",
      "spkts       -0.0\n",
      "dpkts        0.0\n",
      "sbytes      -0.0\n",
      "dbytes       0.0\n",
      "            ... \n",
      "state_PAR   -0.0\n",
      "state_REQ    0.0\n",
      "state_RST   -0.0\n",
      "state_URN    0.0\n",
      "state_no     0.0\n",
      "Length: 197, dtype: float64\n",
      "dur          1.0\n",
      "spkts        1.0\n",
      "dpkts        1.0\n",
      "sbytes       1.0\n",
      "dbytes       1.0\n",
      "            ... \n",
      "state_PAR    1.0\n",
      "state_REQ    1.0\n",
      "state_RST    1.0\n",
      "state_URN    1.0\n",
      "state_no     1.0\n",
      "Length: 197, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Drop irrelevant columns ---\n",
    "df = df.drop(columns=['id', 'attack_cat'])\n",
    "\n",
    "# --- 2. Feature engineering BEFORE encoding/scaling ---\n",
    "df['byte_ratio'] = df['sbytes'] / (df['dbytes'] + 1)\n",
    "df['is_common_port'] = df['ct_dst_sport_ltm'].isin([80, 443, 22]).astype(int)\n",
    "df['flow_intensity'] = (df['spkts'] + df['dpkts']) / (df['dur'] + 1e-6)\n",
    "\n",
    "# --- 3. One-hot encode categorical columns ---\n",
    "categorical_cols = ['proto', 'service', 'state']\n",
    "df = pd.get_dummies(df, columns=categorical_cols)\n",
    "\n",
    "# Convert booleans to ints\n",
    "df = df.astype({col: 'int' for col in df.columns if df[col].dtype == 'bool'})\n",
    "\n",
    "# --- 4. Scale numerical features (except label) ---\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "numerical_cols.remove('label')\n",
    "\n",
    "scaler = StandardScaler()\n",
    "df[numerical_cols] = scaler.fit_transform(df[numerical_cols])\n",
    "\n",
    "# --- Checks ---\n",
    "print(df.shape)                                  # Final number of rows & columns\n",
    "print(df.head())                                 # Preview first few rows\n",
    "print(df.describe().T[['mean', 'std']])          # Confirm scaling stats\n",
    "print(df[numerical_cols].mean().round(3))        # Should be ~0\n",
    "print(df[numerical_cols].std().round(3))         # Should be ~1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5d3766",
   "metadata": {},
   "source": [
    "## Step 3: Upload Preprocessed Data to S3\n",
    "\n",
    "Save the processed data to S3 for use in subsequent steps:\n",
    "- **Local file**: `preprocessed_data.csv`\n",
    "- **S3 destination**: `s3://cybersecurity-data-for-training/processed-data/`\n",
    "- **Purpose**: Store the cleaned, engineered, and scaled features for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62a254fb-5ae1-480e-a9ee-0b3fb51b3c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n",
      "Preprocessed data uploaded to: s3://cybersecurity-data-for-training/processed-data/preprocessed_data.csv\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "# Create SageMaker session and define bucket\n",
    "session = sagemaker.Session()\n",
    "bucket = 'cybersecurity-data-for-training'  # Replace with your actual S3 bucket name\n",
    "processed_prefix = 'processed-data'      # Folder in S3 to store processed files\n",
    "\n",
    "# Save preprocessed data locally\n",
    "df.to_csv('preprocessed_data.csv', index=False)\n",
    "\n",
    "# Upload to S3 inside the 'processed-data/' folder\n",
    "s3_path = session.upload_data(\n",
    "    path='preprocessed_data.csv',\n",
    "    bucket=bucket,\n",
    "    key_prefix=processed_prefix\n",
    ")\n",
    "\n",
    "print(f\"Preprocessed data uploaded to: {s3_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28b6072",
   "metadata": {},
   "source": [
    "## Step 4: Download Preprocessed Data from S3\n",
    "\n",
    "Retrieve the preprocessed data from S3 for further processing:\n",
    "- **Source**: `s3://cybersecurity-data-for-training/processed-data/preprocessed_data.csv`\n",
    "- **Purpose**: Load the clean dataset to prepare for train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea77cf92-d602-4f46-9e8f-61f2932ec44c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dur</th>\n",
       "      <th>spkts</th>\n",
       "      <th>dpkts</th>\n",
       "      <th>sbytes</th>\n",
       "      <th>dbytes</th>\n",
       "      <th>rate</th>\n",
       "      <th>sttl</th>\n",
       "      <th>dttl</th>\n",
       "      <th>sload</th>\n",
       "      <th>dload</th>\n",
       "      <th>...</th>\n",
       "      <th>service_ssl</th>\n",
       "      <th>state_CON</th>\n",
       "      <th>state_ECO</th>\n",
       "      <th>state_FIN</th>\n",
       "      <th>state_INT</th>\n",
       "      <th>state_PAR</th>\n",
       "      <th>state_REQ</th>\n",
       "      <th>state_RST</th>\n",
       "      <th>state_URN</th>\n",
       "      <th>state_no</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.191029</td>\n",
       "      <td>-0.104456</td>\n",
       "      <td>-0.135769</td>\n",
       "      <td>-0.049134</td>\n",
       "      <td>-0.102726</td>\n",
       "      <td>-0.576371</td>\n",
       "      <td>0.703839</td>\n",
       "      <td>1.578100</td>\n",
       "      <td>-0.389897</td>\n",
       "      <td>-0.273700</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.017874</td>\n",
       "      <td>-0.284764</td>\n",
       "      <td>-0.008273</td>\n",
       "      <td>1.119382</td>\n",
       "      <td>-0.940239</td>\n",
       "      <td>-0.002388</td>\n",
       "      <td>-0.10717</td>\n",
       "      <td>-0.021762</td>\n",
       "      <td>-0.002388</td>\n",
       "      <td>-0.002388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.109485</td>\n",
       "      <td>-0.046014</td>\n",
       "      <td>0.172599</td>\n",
       "      <td>-0.046410</td>\n",
       "      <td>0.188544</td>\n",
       "      <td>-0.576345</td>\n",
       "      <td>-1.141901</td>\n",
       "      <td>1.560002</td>\n",
       "      <td>-0.389928</td>\n",
       "      <td>-0.069233</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.017874</td>\n",
       "      <td>-0.284764</td>\n",
       "      <td>-0.008273</td>\n",
       "      <td>1.119382</td>\n",
       "      <td>-0.940239</td>\n",
       "      <td>-0.002388</td>\n",
       "      <td>-0.10717</td>\n",
       "      <td>-0.021762</td>\n",
       "      <td>-0.002388</td>\n",
       "      <td>-0.002388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.040699</td>\n",
       "      <td>-0.089845</td>\n",
       "      <td>-0.026933</td>\n",
       "      <td>-0.048527</td>\n",
       "      <td>-0.012133</td>\n",
       "      <td>-0.576734</td>\n",
       "      <td>-1.141901</td>\n",
       "      <td>1.560002</td>\n",
       "      <td>-0.389964</td>\n",
       "      <td>-0.252044</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.017874</td>\n",
       "      <td>-0.284764</td>\n",
       "      <td>-0.008273</td>\n",
       "      <td>1.119382</td>\n",
       "      <td>-0.940239</td>\n",
       "      <td>-0.002388</td>\n",
       "      <td>-0.10717</td>\n",
       "      <td>-0.021762</td>\n",
       "      <td>-0.002388</td>\n",
       "      <td>-0.002388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.049729</td>\n",
       "      <td>-0.060624</td>\n",
       "      <td>-0.063212</td>\n",
       "      <td>-0.047016</td>\n",
       "      <td>-0.098563</td>\n",
       "      <td>-0.576737</td>\n",
       "      <td>-1.141901</td>\n",
       "      <td>1.560002</td>\n",
       "      <td>-0.389958</td>\n",
       "      <td>-0.275821</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.017874</td>\n",
       "      <td>-0.284764</td>\n",
       "      <td>-0.008273</td>\n",
       "      <td>1.119382</td>\n",
       "      <td>-0.940239</td>\n",
       "      <td>-0.002388</td>\n",
       "      <td>-0.10717</td>\n",
       "      <td>-0.021762</td>\n",
       "      <td>-0.002388</td>\n",
       "      <td>-0.002388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.140417</td>\n",
       "      <td>-0.075235</td>\n",
       "      <td>-0.117630</td>\n",
       "      <td>-0.047554</td>\n",
       "      <td>-0.102057</td>\n",
       "      <td>-0.576617</td>\n",
       "      <td>0.723268</td>\n",
       "      <td>1.560002</td>\n",
       "      <td>-0.389927</td>\n",
       "      <td>-0.275561</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.017874</td>\n",
       "      <td>-0.284764</td>\n",
       "      <td>-0.008273</td>\n",
       "      <td>1.119382</td>\n",
       "      <td>-0.940239</td>\n",
       "      <td>-0.002388</td>\n",
       "      <td>-0.10717</td>\n",
       "      <td>-0.021762</td>\n",
       "      <td>-0.002388</td>\n",
       "      <td>-0.002388</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 198 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        dur     spkts     dpkts    sbytes    dbytes      rate      sttl  \\\n",
       "0 -0.191029 -0.104456 -0.135769 -0.049134 -0.102726 -0.576371  0.703839   \n",
       "1 -0.109485 -0.046014  0.172599 -0.046410  0.188544 -0.576345 -1.141901   \n",
       "2  0.040699 -0.089845 -0.026933 -0.048527 -0.012133 -0.576734 -1.141901   \n",
       "3  0.049729 -0.060624 -0.063212 -0.047016 -0.098563 -0.576737 -1.141901   \n",
       "4 -0.140417 -0.075235 -0.117630 -0.047554 -0.102057 -0.576617  0.723268   \n",
       "\n",
       "       dttl     sload     dload  ...  service_ssl  state_CON  state_ECO  \\\n",
       "0  1.578100 -0.389897 -0.273700  ...    -0.017874  -0.284764  -0.008273   \n",
       "1  1.560002 -0.389928 -0.069233  ...    -0.017874  -0.284764  -0.008273   \n",
       "2  1.560002 -0.389964 -0.252044  ...    -0.017874  -0.284764  -0.008273   \n",
       "3  1.560002 -0.389958 -0.275821  ...    -0.017874  -0.284764  -0.008273   \n",
       "4  1.560002 -0.389927 -0.275561  ...    -0.017874  -0.284764  -0.008273   \n",
       "\n",
       "   state_FIN  state_INT  state_PAR  state_REQ  state_RST  state_URN  state_no  \n",
       "0   1.119382  -0.940239  -0.002388   -0.10717  -0.021762  -0.002388 -0.002388  \n",
       "1   1.119382  -0.940239  -0.002388   -0.10717  -0.021762  -0.002388 -0.002388  \n",
       "2   1.119382  -0.940239  -0.002388   -0.10717  -0.021762  -0.002388 -0.002388  \n",
       "3   1.119382  -0.940239  -0.002388   -0.10717  -0.021762  -0.002388 -0.002388  \n",
       "4   1.119382  -0.940239  -0.002388   -0.10717  -0.021762  -0.002388 -0.002388  \n",
       "\n",
       "[5 rows x 198 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "# Set up session and bucket\n",
    "session = sagemaker.Session()\n",
    "bucket = 'cybersecurity-data-for-training'\n",
    "processed_prefix = 'processed-data'\n",
    "\n",
    "# Download preprocessed data from S3\n",
    "s3 = boto3.client('s3')\n",
    "file_name = 'preprocessed_data.csv'\n",
    "s3.download_file(bucket, f'{processed_prefix}/{file_name}', file_name)\n",
    "\n",
    "# Load into pandas\n",
    "df = pd.read_csv(file_name)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4749b4a0",
   "metadata": {},
   "source": [
    "## Step 5: Train-Test Split & Format Conversion\n",
    "\n",
    "Prepare data for SageMaker XGBoost training:\n",
    "1. **Train-test split**: 80% training, 20% testing (random_state=42 for reproducibility)\n",
    "2. **CSV format**: Save train/test data with labels for inspection\n",
    "3. **LIBSVM format**: Convert to LIBSVM format required by SageMaker's XGBoost algorithm\n",
    "   - This format is memory-efficient and native to XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04d8711f-abad-45e7-8827-7cf149b1d811",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import dump_svmlight_file\n",
    "import pandas as pd\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('preprocessed_data.csv')\n",
    "X = df.drop(columns=['label'])\n",
    "y = df['label']\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# CSV for inspection\n",
    "train_df = pd.concat([y_train, X_train], axis=1)\n",
    "test_df = pd.concat([y_test, X_test], axis=1)\n",
    "train_df.to_csv('train.csv', index=False)\n",
    "test_df.to_csv('test.csv', index=False)\n",
    "\n",
    "# LIBSVM for SageMaker - fixed version\n",
    "dump_svmlight_file(X_train, y_train.values.ravel(), 'train.libsvm')\n",
    "dump_svmlight_file(X_test, y_test.values.ravel(), 'test.libsvm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7cda00",
   "metadata": {},
   "source": [
    "## Step 6: Upload Training & Test Data to S3\n",
    "\n",
    "Upload formatted data to S3 for SageMaker training:\n",
    "- **Training data**: `s3://cybersecurity-data-for-training/xgboost-data/train/train.libsvm`\n",
    "- **Test data**: `s3://cybersecurity-data-for-training/xgboost-data/test/test.libsvm`\n",
    "- **Purpose**: Make data accessible to SageMaker training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29cb9c25-3cd5-4a01-a6fa-ff444e67016c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: s3://cybersecurity-data-for-training/xgboost-data/train/train.libsvm\n",
      "Testing data: s3://cybersecurity-data-for-training/xgboost-data/test/test.libsvm\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "\n",
    "session = sagemaker.Session()\n",
    "bucket = 'cybersecurity-data-for-training'\n",
    "train_prefix = 'xgboost-data/train'\n",
    "test_prefix = 'xgboost-data/test'\n",
    "\n",
    "train_input = session.upload_data('train.libsvm', bucket=bucket, key_prefix=train_prefix)\n",
    "test_input = session.upload_data('test.libsvm', bucket=bucket, key_prefix=test_prefix)\n",
    "\n",
    "print(f\"Training data: {train_input}\")\n",
    "print(f\"Testing data: {test_input}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758ffea9",
   "metadata": {},
   "source": [
    "## Step 7: Configure XGBoost Estimator\n",
    "\n",
    "Set up the SageMaker XGBoost training job with hyperparameters:\n",
    "- **Objective**: `binary:logistic` (binary classification with logistic output)\n",
    "- **Key hyperparameters**:\n",
    "  - `num_round`: 100 boosting rounds\n",
    "  - `max_depth`: 5 (tree depth to prevent overfitting)\n",
    "  - `eta`: 0.2 (learning rate)\n",
    "  - `gamma`: 4 (minimum loss reduction for splitting)\n",
    "  - `subsample`: 0.8 (fraction of samples used per tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3aeeb868-e728-44d8-b6ea-d41e6d00dbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import image_uris\n",
    "from sagemaker.estimator import Estimator\n",
    "\n",
    "xgboost_image_uri = image_uris.retrieve(\"xgboost\", region=session.boto_region_name, version=\"1.3-1\")\n",
    "\n",
    "xgb = Estimator(\n",
    "    image_uri=xgboost_image_uri,\n",
    "    role=sagemaker.get_execution_role(),\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.large',\n",
    "    output_path=f's3://cybersecurity-data-for-training/xgboost-model-output',\n",
    "    sagemaker_session=session\n",
    ")\n",
    "\n",
    "xgb.set_hyperparameters(\n",
    "    objective='binary:logistic',\n",
    "    num_round=100,\n",
    "    max_depth=5,\n",
    "    eta=0.2,\n",
    "    gamma=4,\n",
    "    min_child_weight=6,\n",
    "    subsample=0.8,\n",
    "    verbosity=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37c5a88",
   "metadata": {},
   "source": [
    "## Step 8: Train XGBoost Model on SageMaker\n",
    "\n",
    "Execute the training job on SageMaker:\n",
    "- Uses the configured estimator to train on the training data\n",
    "- Validates against test data during training\n",
    "- Model artifacts are saved to S3 upon completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5059901a-2007-4068-84ee-765bcfa334ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: sagemaker-xgboost-2026-01-01-02-46-18-038\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-01 02:46:20 Starting - Starting the training job...\n",
      "2026-01-01 02:46:34 Starting - Preparing the instances for training...\n",
      "2026-01-01 02:46:56 Downloading - Downloading input data......\n",
      "2026-01-01 02:47:56 Downloading - Downloading the training image...\n",
      "2026-01-01 02:48:27 Training - Training image download completed. Training in progress..\u001b[34m[2026-01-01 02:48:31.844 ip-10-0-107-73.ap-southeast-2.compute.internal:7 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2026-01-01 02:48:31.867 ip-10-0-107-73.ap-southeast-2.compute.internal:7 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2026-01-01:02:48:31:INFO] Imported framework sagemaker_xgboost_container.training\u001b[0m\n",
      "\u001b[34m[2026-01-01:02:48:31:INFO] Failed to parse hyperparameter objective value binary:logistic to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m[2026-01-01:02:48:31:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2026-01-01:02:48:31:INFO] Running XGBoost Sagemaker in algorithm mode\u001b[0m\n",
      "\u001b[34m[2026-01-01:02:48:31:INFO] files path: /opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34m[2026-01-01:02:48:35:INFO] files path: /opt/ml/input/data/validation\u001b[0m\n",
      "\u001b[34m[2026-01-01:02:48:36:INFO] Single node training.\u001b[0m\n",
      "\u001b[34m[2026-01-01:02:48:36:INFO] Train matrix has 140272 rows and 197 columns\u001b[0m\n",
      "\u001b[34m[2026-01-01:02:48:36:INFO] Validation matrix has 35069 rows\u001b[0m\n",
      "\u001b[34m[2026-01-01 02:48:36.611 ip-10-0-107-73.ap-southeast-2.compute.internal:7 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2026-01-01 02:48:36.612 ip-10-0-107-73.ap-southeast-2.compute.internal:7 INFO hook.py:207] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2026-01-01 02:48:36.613 ip-10-0-107-73.ap-southeast-2.compute.internal:7 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2026-01-01 02:48:36.613 ip-10-0-107-73.ap-southeast-2.compute.internal:7 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2026-01-01:02:48:36:INFO] Debug hook created from config\u001b[0m\n",
      "\u001b[34m[02:48:38] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\u001b[0m\n",
      "\u001b[34m[2026-01-01 02:48:38.294 ip-10-0-107-73.ap-southeast-2.compute.internal:7 INFO hook.py:428] Monitoring the collections: metrics\u001b[0m\n",
      "\u001b[34m[2026-01-01 02:48:38.297 ip-10-0-107-73.ap-southeast-2.compute.internal:7 INFO hook.py:491] Hook is writing from the hook with pid: 7\u001b[0m\n",
      "\u001b[34m[0]#011train-logloss:0.54325#011validation-logloss:0.54353\u001b[0m\n",
      "\u001b[34m[1]#011train-logloss:0.44245#011validation-logloss:0.44294\u001b[0m\n",
      "\u001b[34m[2]#011train-logloss:0.37077#011validation-logloss:0.37129\u001b[0m\n",
      "\u001b[34m[3]#011train-logloss:0.31652#011validation-logloss:0.31751\u001b[0m\n",
      "\u001b[34m[4]#011train-logloss:0.27522#011validation-logloss:0.27626\u001b[0m\n",
      "\u001b[34m[5]#011train-logloss:0.24343#011validation-logloss:0.24452\u001b[0m\n",
      "\u001b[34m[6]#011train-logloss:0.21877#011validation-logloss:0.22001\u001b[0m\n",
      "\u001b[34m[7]#011train-logloss:0.19919#011validation-logloss:0.20069\u001b[0m\n",
      "\u001b[34m[8]#011train-logloss:0.18347#011validation-logloss:0.18507\u001b[0m\n",
      "\u001b[34m[9]#011train-logloss:0.17097#011validation-logloss:0.17285\u001b[0m\n",
      "\u001b[34m[10]#011train-logloss:0.16029#011validation-logloss:0.16226\u001b[0m\n",
      "\u001b[34m[11]#011train-logloss:0.15220#011validation-logloss:0.15426\u001b[0m\n",
      "\u001b[34m[12]#011train-logloss:0.14537#011validation-logloss:0.14757\u001b[0m\n",
      "\u001b[34m[13]#011train-logloss:0.13981#011validation-logloss:0.14226\u001b[0m\n",
      "\u001b[34m[14]#011train-logloss:0.13543#011validation-logloss:0.13783\u001b[0m\n",
      "\u001b[34m[15]#011train-logloss:0.13200#011validation-logloss:0.13457\u001b[0m\n",
      "\u001b[34m[16]#011train-logloss:0.12881#011validation-logloss:0.13137\u001b[0m\n",
      "\u001b[34m[17]#011train-logloss:0.12573#011validation-logloss:0.12840\u001b[0m\n",
      "\u001b[34m[18]#011train-logloss:0.12386#011validation-logloss:0.12656\u001b[0m\n",
      "\u001b[34m[19]#011train-logloss:0.12194#011validation-logloss:0.12465\u001b[0m\n",
      "\u001b[34m[20]#011train-logloss:0.12007#011validation-logloss:0.12290\u001b[0m\n",
      "\u001b[34m[21]#011train-logloss:0.11867#011validation-logloss:0.12152\u001b[0m\n",
      "\u001b[34m[22]#011train-logloss:0.11779#011validation-logloss:0.12062\u001b[0m\n",
      "\u001b[34m[23]#011train-logloss:0.11664#011validation-logloss:0.11951\u001b[0m\n",
      "\u001b[34m[24]#011train-logloss:0.11565#011validation-logloss:0.11852\u001b[0m\n",
      "\u001b[34m[25]#011train-logloss:0.11458#011validation-logloss:0.11749\u001b[0m\n",
      "\u001b[34m[26]#011train-logloss:0.11398#011validation-logloss:0.11697\u001b[0m\n",
      "\u001b[34m[27]#011train-logloss:0.11264#011validation-logloss:0.11546\u001b[0m\n",
      "\u001b[34m[28]#011train-logloss:0.11173#011validation-logloss:0.11458\u001b[0m\n",
      "\u001b[34m[29]#011train-logloss:0.11042#011validation-logloss:0.11319\u001b[0m\n",
      "\u001b[34m[30]#011train-logloss:0.10990#011validation-logloss:0.11273\u001b[0m\n",
      "\u001b[34m[31]#011train-logloss:0.10860#011validation-logloss:0.11142\u001b[0m\n",
      "\u001b[34m[32]#011train-logloss:0.10822#011validation-logloss:0.11105\u001b[0m\n",
      "\u001b[34m[33]#011train-logloss:0.10786#011validation-logloss:0.11069\u001b[0m\n",
      "\u001b[34m[34]#011train-logloss:0.10709#011validation-logloss:0.11004\u001b[0m\n",
      "\u001b[34m[35]#011train-logloss:0.10618#011validation-logloss:0.10924\u001b[0m\n",
      "\u001b[34m[36]#011train-logloss:0.10593#011validation-logloss:0.10896\u001b[0m\n",
      "\u001b[34m[37]#011train-logloss:0.10503#011validation-logloss:0.10815\u001b[0m\n",
      "\u001b[34m[38]#011train-logloss:0.10436#011validation-logloss:0.10756\u001b[0m\n",
      "\u001b[34m[39]#011train-logloss:0.10407#011validation-logloss:0.10731\u001b[0m\n",
      "\u001b[34m[40]#011train-logloss:0.10350#011validation-logloss:0.10688\u001b[0m\n",
      "\u001b[34m[41]#011train-logloss:0.10331#011validation-logloss:0.10677\u001b[0m\n",
      "\u001b[34m[42]#011train-logloss:0.10307#011validation-logloss:0.10654\u001b[0m\n",
      "\u001b[34m[43]#011train-logloss:0.10253#011validation-logloss:0.10607\u001b[0m\n",
      "\u001b[34m[44]#011train-logloss:0.10228#011validation-logloss:0.10577\u001b[0m\n",
      "\u001b[34m[45]#011train-logloss:0.10172#011validation-logloss:0.10531\u001b[0m\n",
      "\u001b[34m[46]#011train-logloss:0.10141#011validation-logloss:0.10512\u001b[0m\n",
      "\u001b[34m[47]#011train-logloss:0.10081#011validation-logloss:0.10461\u001b[0m\n",
      "\u001b[34m[48]#011train-logloss:0.10040#011validation-logloss:0.10425\u001b[0m\n",
      "\u001b[34m[49]#011train-logloss:0.10010#011validation-logloss:0.10403\u001b[0m\n",
      "\u001b[34m[50]#011train-logloss:0.09978#011validation-logloss:0.10384\u001b[0m\n",
      "\u001b[34m[51]#011train-logloss:0.09949#011validation-logloss:0.10358\u001b[0m\n",
      "\u001b[34m[52]#011train-logloss:0.09907#011validation-logloss:0.10311\u001b[0m\n",
      "\u001b[34m[53]#011train-logloss:0.09859#011validation-logloss:0.10273\u001b[0m\n",
      "\u001b[34m[54]#011train-logloss:0.09847#011validation-logloss:0.10258\u001b[0m\n",
      "\u001b[34m[55]#011train-logloss:0.09812#011validation-logloss:0.10243\u001b[0m\n",
      "\u001b[34m[56]#011train-logloss:0.09799#011validation-logloss:0.10234\u001b[0m\n",
      "\u001b[34m[57]#011train-logloss:0.09781#011validation-logloss:0.10217\u001b[0m\n",
      "\u001b[34m[58]#011train-logloss:0.09762#011validation-logloss:0.10200\u001b[0m\n",
      "\u001b[34m[59]#011train-logloss:0.09719#011validation-logloss:0.10164\u001b[0m\n",
      "\u001b[34m[60]#011train-logloss:0.09711#011validation-logloss:0.10157\u001b[0m\n",
      "\u001b[34m[61]#011train-logloss:0.09673#011validation-logloss:0.10126\u001b[0m\n",
      "\u001b[34m[62]#011train-logloss:0.09641#011validation-logloss:0.10106\u001b[0m\n",
      "\u001b[34m[63]#011train-logloss:0.09595#011validation-logloss:0.10067\u001b[0m\n",
      "\u001b[34m[64]#011train-logloss:0.09585#011validation-logloss:0.10062\u001b[0m\n",
      "\u001b[34m[65]#011train-logloss:0.09568#011validation-logloss:0.10049\u001b[0m\n",
      "\u001b[34m[66]#011train-logloss:0.09532#011validation-logloss:0.10016\u001b[0m\n",
      "\u001b[34m[67]#011train-logloss:0.09498#011validation-logloss:0.09996\u001b[0m\n",
      "\u001b[34m[68]#011train-logloss:0.09481#011validation-logloss:0.09979\u001b[0m\n",
      "\u001b[34m[69]#011train-logloss:0.09462#011validation-logloss:0.09965\u001b[0m\n",
      "\u001b[34m[70]#011train-logloss:0.09434#011validation-logloss:0.09950\u001b[0m\n",
      "\u001b[34m[71]#011train-logloss:0.09406#011validation-logloss:0.09934\u001b[0m\n",
      "\u001b[34m[72]#011train-logloss:0.09397#011validation-logloss:0.09928\u001b[0m\n",
      "\u001b[34m[73]#011train-logloss:0.09391#011validation-logloss:0.09928\u001b[0m\n",
      "\u001b[34m[74]#011train-logloss:0.09366#011validation-logloss:0.09914\u001b[0m\n",
      "\u001b[34m[75]#011train-logloss:0.09360#011validation-logloss:0.09908\u001b[0m\n",
      "\u001b[34m[76]#011train-logloss:0.09349#011validation-logloss:0.09901\u001b[0m\n",
      "\u001b[34m[77]#011train-logloss:0.09329#011validation-logloss:0.09901\u001b[0m\n",
      "\u001b[34m[78]#011train-logloss:0.09297#011validation-logloss:0.09873\u001b[0m\n",
      "\u001b[34m[79]#011train-logloss:0.09257#011validation-logloss:0.09848\u001b[0m\n",
      "\u001b[34m[80]#011train-logloss:0.09234#011validation-logloss:0.09836\u001b[0m\n",
      "\u001b[34m[81]#011train-logloss:0.09220#011validation-logloss:0.09824\u001b[0m\n",
      "\u001b[34m[82]#011train-logloss:0.09202#011validation-logloss:0.09822\u001b[0m\n",
      "\u001b[34m[83]#011train-logloss:0.09179#011validation-logloss:0.09810\u001b[0m\n",
      "\u001b[34m[84]#011train-logloss:0.09168#011validation-logloss:0.09808\u001b[0m\n",
      "\u001b[34m[85]#011train-logloss:0.09156#011validation-logloss:0.09806\u001b[0m\n",
      "\u001b[34m[86]#011train-logloss:0.09143#011validation-logloss:0.09793\u001b[0m\n",
      "\u001b[34m[87]#011train-logloss:0.09122#011validation-logloss:0.09772\u001b[0m\n",
      "\u001b[34m[88]#011train-logloss:0.09098#011validation-logloss:0.09765\u001b[0m\n",
      "\u001b[34m[89]#011train-logloss:0.09074#011validation-logloss:0.09750\u001b[0m\n",
      "\u001b[34m[90]#011train-logloss:0.09068#011validation-logloss:0.09749\u001b[0m\n",
      "\u001b[34m[91]#011train-logloss:0.09041#011validation-logloss:0.09733\u001b[0m\n",
      "\u001b[34m[92]#011train-logloss:0.09033#011validation-logloss:0.09729\u001b[0m\n",
      "\u001b[34m[93]#011train-logloss:0.09029#011validation-logloss:0.09729\u001b[0m\n",
      "\u001b[34m[94]#011train-logloss:0.09007#011validation-logloss:0.09711\u001b[0m\n",
      "\u001b[34m[95]#011train-logloss:0.08977#011validation-logloss:0.09691\u001b[0m\n",
      "\u001b[34m[96]#011train-logloss:0.08959#011validation-logloss:0.09680\u001b[0m\n",
      "\u001b[34m[97]#011train-logloss:0.08952#011validation-logloss:0.09682\u001b[0m\n",
      "\u001b[34m[98]#011train-logloss:0.08936#011validation-logloss:0.09680\u001b[0m\n",
      "\u001b[34m[99]#011train-logloss:0.08920#011validation-logloss:0.09675\u001b[0m\n",
      "\n",
      "2026-01-01 02:50:07 Uploading - Uploading generated training model\n",
      "2026-01-01 02:50:26 Completed - Training job completed\n",
      "Training seconds: 210\n",
      "Billable seconds: 210\n"
     ]
    }
   ],
   "source": [
    "# Train using data channels\n",
    "xgb.fit({'train': train_input, 'validation': test_input})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a33ae3",
   "metadata": {},
   "source": [
    "## Step 9: Install XGBoost Library Locally\n",
    "\n",
    "Install the specific version of XGBoost needed for local training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d39c24a-48d3-4fd8-84f5-a4b958fbd919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost==1.7.6\n",
      "  Downloading xgboost-1.7.6-py3-none-manylinux2014_x86_64.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from xgboost==1.7.6) (1.26.4)\n",
      "Requirement already satisfied: scipy in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from xgboost==1.7.6) (1.15.2)\n",
      "Downloading xgboost-1.7.6-py3-none-manylinux2014_x86_64.whl (200.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.3/200.3 MB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m  \u001b[33m0:00:12\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: xgboost\n",
      "Successfully installed xgboost-1.7.6\n"
     ]
    }
   ],
   "source": [
    "!pip install -U \"xgboost==1.7.6\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880467da",
   "metadata": {},
   "source": [
    "## Step 10: Evaluate Model Performance\n",
    "\n",
    "Once training is complete, train the XGBoost model locally and evaluate its performance:\n",
    "1. **Load data**: Read train/test CSV files from disk\n",
    "2. **Data preparation**: Convert to numeric format and handle missing values\n",
    "3. **DMatrix conversion**: Convert data to XGBoost's native DMatrix structure\n",
    "4. **Model training**: Train XGBoost locally using the same hyperparameters as SageMaker training\n",
    "5. **Performance evaluation**: \n",
    "   - Generate probability predictions on test set\n",
    "   - Calculate accuracy score\n",
    "   - Generate classification report with precision, recall, and F1-score metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15a2e59b-1e7f-4c94-89f2-0cc5b79c0fe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9560865721862614\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      0.91      0.93     11169\n",
      "         1.0       0.96      0.98      0.97     23900\n",
      "\n",
      "    accuracy                           0.96     35069\n",
      "   macro avg       0.95      0.94      0.95     35069\n",
      "weighted avg       0.96      0.96      0.96     35069\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load and convert data\n",
    "train_data = pd.read_csv('train.csv', header=None, dtype=str)\n",
    "test_data = pd.read_csv('test.csv', header=None, dtype=str)\n",
    "\n",
    "# Convert all columns to numeric\n",
    "train_data = train_data.apply(pd.to_numeric, errors='coerce')\n",
    "test_data = test_data.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Drop any rows with NaNs\n",
    "train_data = train_data.dropna()\n",
    "test_data = test_data.dropna()\n",
    "\n",
    "# Split into features (X) and labels (y)\n",
    "X_train = train_data.iloc[:, 1:]\n",
    "y_train = train_data.iloc[:, 0]\n",
    "X_test = test_data.iloc[:, 1:]\n",
    "y_test = test_data.iloc[:, 0]\n",
    "\n",
    "# Convert to DMatrix format\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test)\n",
    "\n",
    "# Set parameters and train the model\n",
    "params = {\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"max_depth\": 5,\n",
    "    \"eta\": 0.2,\n",
    "    \"gamma\": 4,\n",
    "    \"min_child_weight\": 6,\n",
    "    \"subsample\": 0.8,\n",
    "    \"verbosity\": 1\n",
    "}\n",
    "\n",
    "model = xgb.train(params=params, dtrain=dtrain, num_boost_round=100)\n",
    "\n",
    "# Predict\n",
    "y_pred_prob = model.predict(dtest)\n",
    "y_pred = [1 if p > 0.5 else 0 for p in y_pred_prob]\n",
    "\n",
    "# Evaluate\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25de82f",
   "metadata": {},
   "source": [
    "## Step 11: Register Model in SageMaker\n",
    "\n",
    "Register the trained XGBoost model in SageMaker Model Registry:\n",
    "- **Model name**: `cybersecurity-threat-xgboost`\n",
    "- **Model artifact**: Points to the trained model in S3\n",
    "- **Purpose**: Make the model available for endpoint deployment\n",
    "- **Region**: ap-southeast-2 (Australia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "804c4f1c-a8af-4883-b4d9-11d97c76c83a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model cybersecurity-threat-xgboost registered successfully in SageMaker!\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "from sagemaker import image_uris\n",
    "\n",
    "sagemaker_client = boto3.client(\"sagemaker\")\n",
    "region = \"ap-southeast-2\"\n",
    "bucket_name = \"cybersecurity-data-for-training\"\n",
    "model_artifact = f\"s3://cybersecurity-data-for-training/xgboost-model-output/sagemaker-xgboost-2026-01-01-02-46-18-038/output/model.tar.gz\"\n",
    "model_name = \"cybersecurity-threat-xgboost\"\n",
    "\n",
    "# Get XGBoost image URI\n",
    "image_uri = image_uris.retrieve(\"xgboost\", region=region, version=\"1.3-1\")\n",
    "\n",
    "# Use actual IAM Role ARN\n",
    "execution_role = \"arn:aws:iam::152141418178:role/SageMakerCybersecurityRole\"\n",
    "\n",
    "# Register the model\n",
    "response = sagemaker_client.create_model(\n",
    "    ModelName=model_name,\n",
    "    PrimaryContainer={\n",
    "        \"Image\": image_uri,\n",
    "        \"ModelDataUrl\": model_artifact\n",
    "    },\n",
    "    ExecutionRoleArn=execution_role\n",
    ")\n",
    "\n",
    "print(f\"Model {model_name} registered successfully in SageMaker!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ae7595",
   "metadata": {},
   "source": [
    "## Step 12: Create Endpoint Configuration & Deploy\n",
    "\n",
    "Deploy the registered model as a real-time inference endpoint:\n",
    "1. **Endpoint config**: Define the production variant with model and instance type\n",
    "2. **Instance type**: `ml.m5.large` (general-purpose, cost-effective)\n",
    "3. **Deployment**: Create the endpoint (this may take several minutes to provision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "117db14f-36cc-4ab6-a5e0-b9381e461afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint 'cybersecurity-threat-endpoint' is being deployed. This may take a few minutes...\n"
     ]
    }
   ],
   "source": [
    "# Define model name if not already defined\n",
    "model_name = \"cybersecurity-threat-xgboost\"\n",
    "\n",
    "# Define endpoint configuration\n",
    "endpoint_config_name = \"cybersecurity-threat-config\"\n",
    "\n",
    "sagemaker_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"VariantName\": \"DefaultVariant\",\n",
    "            \"ModelName\": model_name,\n",
    "            \"InstanceType\": \"ml.m5.large\",\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"InitialVariantWeight\": 1\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Deploy endpoint\n",
    "endpoint_name = \"cybersecurity-threat-endpoint\"\n",
    "\n",
    "sagemaker_client.create_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    EndpointConfigName=endpoint_config_name\n",
    ")\n",
    "\n",
    "print(f\"Endpoint '{endpoint_name}' is being deployed. This may take a few minutes...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa51d766",
   "metadata": {},
   "source": [
    "## Step 13: Test Deployed Endpoint with Real-Time Inference\n",
    "\n",
    "Invoke the deployed endpoint with sample data to verify it's working:\n",
    "- **Input**: 8 feature values in CSV format (normalized to [-3, +3] range)\n",
    "- **Output**: Probability score from the model\n",
    "- **Prediction**: Classify as \"THREAT\" if score > 0.5, otherwise \"SAFE\"\n",
    "- **Purpose**: End-to-end validation of the deployed model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "89215f94-ad47-4fab-8231-e00868056da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: THREAT\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import numpy as np\n",
    "\n",
    "runtime_client = boto3.client(\"sagemaker-runtime\")\n",
    "\n",
    "# Sample input in CSV format\n",
    "sample_input = \"0.5,0.3,0.8,0.2,0.1,0.6,0.9,0.4\"\n",
    "\n",
    "# Invoke the endpoint\n",
    "response = runtime_client.invoke_endpoint(\n",
    "    EndpointName=\"cybersecurity-threat-endpoint\",  # or use endpoint_name if defined\n",
    "    ContentType=\"text/csv\",\n",
    "    Body=sample_input\n",
    ")\n",
    "\n",
    "# Get prediction from response\n",
    "result = response[\"Body\"].read().decode(\"utf-8\")\n",
    "prediction_score = float(result.strip())\n",
    "\n",
    "# Interpret prediction\n",
    "predicted_label = \"THREAT\" if prediction_score > 0.5 else \"SAFE\"\n",
    "\n",
    "print(f\"Prediction: {predicted_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5d0d03-063d-43e3-a26e-51415d04c3bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
